"""Telemetry collection and feature extraction helpers for Phase 4 ML integration.

This module loads JSONL telemetry captured by the sandbox monitor (both real and
synthetic runs) and computes feature vectors that downstream models can
consume. The implementation favours readability and defensive handling over raw
performance because datasets are relatively small (hundreds to thousands of
runs).
"""

from __future__ import annotations

import datetime as dt
import json
import statistics
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, MutableMapping, Optional, Sequence

import numpy as np

REAL_SOURCE = "real"
SYNTH_SOURCE = "synthetic"
UNKNOWN_LABEL = "unknown"
BENIGN_LABEL = "benign"
MALICIOUS_LABEL = "malicious"


@dataclass(slots=True)
class TelemetryRun:
    """Normalized representation of a single sandbox run."""

    run_id: str
    path: Path
    source: str
    start_event: Dict[str, object]
    samples: List[MutableMapping[str, object]]
    stop_event: Optional[Dict[str, object]] = None
    label: Optional[str] = None
    summary: Optional[str] = None

    def duration_seconds(self) -> float:
        if self.stop_event and "duration_seconds" in self.stop_event:
            try:
                return float(self.stop_event["duration_seconds"])  # type: ignore[arg-type]
            except Exception:
                pass
        interval = _extract_interval(self.start_event)
        if interval <= 0:
            interval = _infer_interval(self.samples)
        return interval * max(len(self.samples), 1)

    def violation_count(self) -> int:
        if self.stop_event and "violation_count" in self.stop_event:
            try:
                return int(self.stop_event["violation_count"])  # type: ignore[arg-type]
            except Exception:
                return 0
        if self.stop_event and "violations" in self.stop_event:
            value = self.stop_event["violations"]
            if isinstance(value, Sequence) and not isinstance(value, (str, bytes)):
                return len(value)
        return 0


@dataclass(slots=True)
class FeatureVector:
    """Feature vector derived from a telemetry run."""

    run: TelemetryRun
    features: Dict[str, float]
    label: str


def collect_runs(
    log_dir: Path,
    synthetic_dir: Optional[Path] = None,
) -> List[TelemetryRun]:
    """Load telemetry runs from disk.

    Parameters
    ----------
    log_dir:
        Directory containing `monitor_run_*.jsonl` files captured by the live
        monitor.
    synthetic_dir:
        Optional directory containing synthetic JSONL telemetry generated by
        `data.sample_generator`.
    """

    runs: List[TelemetryRun] = []
    log_dir = log_dir.expanduser().resolve()
    synthetic_paths: List[Path] = []

    for path in sorted(log_dir.glob("monitor_run_*.jsonl")):
        run = _load_run(path, source=REAL_SOURCE)
        if run:
            runs.append(run)

    if synthetic_dir is not None:
        synthetic_dir = synthetic_dir.expanduser().resolve()
        for path in sorted(synthetic_dir.glob("*.jsonl")):
            synthetic_paths.append(path)
        for path in synthetic_paths:
            run = _load_run(path, source=SYNTH_SOURCE)
            if run:
                runs.append(run)

    return runs


def compute_features(run: TelemetryRun) -> FeatureVector:
    samples = run.samples
    if not samples:
        features = _empty_feature_vector()
        return FeatureVector(run=run, features=features, label=run.label or UNKNOWN_LABEL)

    timestamps, cpu_vals = _extract_series(samples, "cpu_percent")
    _, rss_vals = _extract_series(samples, "memory_rss")

    _, read_bytes = _extract_series(samples, "read_bytes")
    _, write_bytes = _extract_series(samples, "write_bytes")
    _, open_files = _extract_series(samples, "open_files")
    _, socket_counts = _extract_series(samples, "socket_count")

    interval = _extract_interval(run.start_event)
    if interval <= 0:
        interval = _infer_interval(samples)
    duration = run.duration_seconds() or interval * len(samples)

    cpu_mean = float(np.mean(cpu_vals)) if cpu_vals.size > 0 else 0.0
    cpu_max = float(np.max(cpu_vals)) if cpu_vals.size > 0 else 0.0
    cpu_std = float(np.std(cpu_vals)) if cpu_vals.size > 1 else 0.0
    cpu_slope = _compute_slope(timestamps, cpu_vals)

    rss_mean = float(np.mean(rss_vals)) if rss_vals.size > 0 else 0.0
    rss_max = float(np.max(rss_vals)) if rss_vals.size > 0 else 0.0
    rss_std = float(np.std(rss_vals)) if rss_vals.size > 1 else 0.0
    rss_slope = _compute_slope(timestamps, rss_vals)

    read_rate = _compute_rate(read_bytes, duration)
    write_rate = _compute_rate(write_bytes, duration)

    open_files_mean = float(np.mean(open_files)) if open_files.size > 0 else 0.0
    socket_count_mean = float(np.mean(socket_counts)) if socket_counts.size > 0 else 0.0

    time_above_cpu_50 = _time_above_threshold(cpu_vals, interval, threshold=50.0)

    violation_count = float(run.violation_count())

    features = {
        "cpu_mean": cpu_mean,
        "cpu_max": cpu_max,
        "cpu_std": cpu_std,
        "cpu_slope": cpu_slope,
        "rss_mean": rss_mean,
        "rss_max": rss_max,
        "rss_std": rss_std,
        "rss_slope": rss_slope,
        "io_read_rate": read_rate,
        "io_write_rate": write_rate,
        "open_files_mean": open_files_mean,
        "socket_count_mean": socket_count_mean,
        "time_above_cpu_50": time_above_cpu_50,
        "violation_count": violation_count,
        "duration_seconds": float(duration),
        "threads_mean": float(np.mean(_extract_numeric(samples, "threads"))) if samples else 0.0,
    }

    label = run.label or (run.stop_event or {}).get("label") or UNKNOWN_LABEL
    if isinstance(label, str):
        label = label.lower()
    else:
        label = UNKNOWN_LABEL

    return FeatureVector(run=run, features=features, label=str(label))


def build_feature_table(runs: Iterable[TelemetryRun]) -> List[FeatureVector]:
    return [compute_features(run) for run in runs]


def _load_run(path: Path, source: str) -> Optional[TelemetryRun]:
    try:
        events = list(_read_jsonl(path))
    except OSError:
        return None
    if not events:
        return None

    start_event = next((event for event in events if event.get("event") == "start"), None)
    samples = [event for event in events if event.get("event") == "sample"]
    stop_event = next((event for event in reversed(events) if event.get("event") == "stop"), None)

    if start_event is None:
        start_event = {"event": "start", "timestamp": dt.datetime.now(dt.timezone.utc).isoformat()}

    label = None
    summary = None
    if stop_event:
        label = stop_event.get("label") if isinstance(stop_event.get("label"), str) else None
        summary = stop_event.get("summary") if isinstance(stop_event.get("summary"), str) else None

    run_id = path.stem
    return TelemetryRun(
        run_id=run_id,
        path=path,
        source=source,
        start_event=start_event,
        samples=[_normalise_sample(sample) for sample in samples],
        stop_event=stop_event,
        label=label,
        summary=summary,
    )


def _read_jsonl(path: Path) -> Iterator[Dict[str, object]]:
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            line = line.strip()
            if not line:
                continue
            try:
                yield json.loads(line)
            except json.JSONDecodeError:
                continue


def _normalise_sample(sample: MutableMapping[str, object]) -> MutableMapping[str, object]:
    defaults = {
        "cpu_percent": 0.0,
        "memory_rss": 0.0,
        "memory_vms": 0.0,
        "threads": 0,
        "open_files": 0,
        "socket_count": 0,
        "read_bytes": 0,
        "write_bytes": 0,
    }
    for key, default in defaults.items():
        value = sample.get(key, default)
        if value is None:
            sample[key] = default
        else:
            try:
                sample[key] = float(value) if isinstance(default, float) else int(value)
            except (TypeError, ValueError):
                sample[key] = default
    return sample


def _extract_series(samples: List[MutableMapping[str, object]], key: str) -> tuple[np.ndarray, np.ndarray]:
    timestamps: List[float] = []
    values: List[float] = []
    for sample in samples:
        ts_raw = sample.get("timestamp")
        if isinstance(ts_raw, str):
            timestamps.append(_to_epoch(ts_raw))
        else:
            timestamps.append(float(len(timestamps)))
        raw_value = sample.get(key)
        try:
            values.append(float(raw_value))
        except (TypeError, ValueError):
            values.append(0.0)
    return np.array(timestamps, dtype=float), np.array(values, dtype=float)


def _extract_interval(start_event: Dict[str, object]) -> float:
    interval = start_event.get("interval")
    if isinstance(interval, (int, float)):
        return float(interval)
    try:
        return float(interval)  # type: ignore[arg-type]
    except Exception:
        return 0.0


def _infer_interval(samples: List[MutableMapping[str, object]]) -> float:
    if len(samples) < 2:
        return 0.2
    timestamps = []
    for sample in samples[:5]:
        ts = sample.get("timestamp")
        if isinstance(ts, str):
            timestamps.append(_to_epoch(ts))
    if len(timestamps) < 2:
        return 0.2
    diffs = [max(timestamps[i + 1] - timestamps[i], 1e-6) for i in range(len(timestamps) - 1)]
    try:
        return statistics.mean(diffs)
    except statistics.StatisticsError:
        return 0.2


def _compute_slope(timestamps: np.ndarray, values: np.ndarray) -> float:
    if len(values) < 2:
        return 0.0
    if np.allclose(values, values[0]):
        return 0.0
    try:
        coeffs = np.polyfit(timestamps, values, 1)
    except np.linalg.LinAlgError:
        return 0.0
    return float(coeffs[0])


def _compute_rate(series: np.ndarray, duration: float) -> float:
    if len(series) < 2 or duration <= 0:
        return 0.0
    total = float(series[-1] - series[0])
    return total / duration


def _time_above_threshold(values: np.ndarray, interval: float, threshold: float) -> float:
    if interval <= 0:
        interval = 0.2
    count = float(np.sum(values > threshold))
    return count * interval


def _extract_numeric(samples: List[MutableMapping[str, object]], key: str) -> List[float]:
    values: List[float] = []
    for sample in samples:
        value = sample.get(key)
        try:
            values.append(float(value))
        except (TypeError, ValueError):
            continue
    return values


def _empty_feature_vector() -> Dict[str, float]:
    return {
        "cpu_mean": 0.0,
        "cpu_max": 0.0,
        "cpu_std": 0.0,
        "cpu_slope": 0.0,
        "rss_mean": 0.0,
        "rss_max": 0.0,
        "rss_std": 0.0,
        "rss_slope": 0.0,
        "io_read_rate": 0.0,
        "io_write_rate": 0.0,
        "open_files_mean": 0.0,
        "socket_count_mean": 0.0,
        "time_above_cpu_50": 0.0,
        "violation_count": 0.0,
        "duration_seconds": 0.0,
        "threads_mean": 0.0,
    }


def _to_epoch(ts: str) -> float:
    ts = ts.replace("Z", "+00:00")
    try:
        return dt.datetime.fromisoformat(ts).timestamp()
    except ValueError:
        return float(len(ts))